---
title: "Autograph Basics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autograph-basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
reticulate::use_virtualenv("tf2", TRUE)
```

```{r setup}
library(magrittr)
library(reticulate)
library(tensorflow)
library(tfdatasets)

library(tfautograph)
tf$version$VERSION
```

The R package `tfautograph` helps you write natural R code for tensorflow. 

It allows you to use tensors in R control flow expressions like `if`, `while`,
`for`, `break`, and `next`, which can automatically be translated to build a
tensorflow graph  (hence the name, __auto__ __graph__). 

However tfautograph doesn't just work in graph mode--it also works in eager mode. In addition, it provides a handful of thin wrappers around tensorflow control primitives
and some S3 methods for `TensorArrays` that make their use from R a little more
ergonomic.

This vignette goes through some of the main features and then goes into how it
works a little.

### Usage

The primary workhorse function of the package is the function `autograph()`. For
most use-cases, this is the only function from the package you will need or want
to call. It can either take a function or an expression. The following two uses
are equivalent.

```{r}
# pass a function to autograph()
fn <- function(x) if(x > 0) x * x else x
square_if_positive <- autograph(fn) 

# pass an expression to autograph()
square_if_positive <- autograph(function(x) if(x > 0) x * x else x)
```

Now `square_if_positive` is a function that can accept a tensor as an argument. 

```{r}
x <- tf$convert_to_tensor(5)
y <- tf$convert_to_tensor(-5)
square_if_positive(x)
square_if_positive(y)
```

Note that if you're in a context where tensorflow is executing eagerly, `autograph()` doesn't change that--`square_if_positive()` is still executing eagerly. You can test that by inserting some R `print` statements to see when a branch is evaluated.

```{r}
square_if_positive_verbose <- autograph(function(x) {
  if (x > 0) {
    message("Tracing true branch")
    x * x
  } else {
    message("Tracing false branch")
    x
  }
})

square_if_positive_verbose(x)
square_if_positive_verbose(x)
square_if_positive_verbose(x)

square_if_positive_verbose(y)
square_if_positive_verbose(y)
square_if_positive_verbose(y)
```

The easiest way to enter a context where tensorflow is not executing eagerly anymore and instead is in **graph mode** is to call python's `tf.function()`. (Because `function` is a reserved word for the R parser, there is a convenient wrapper provided by the tensorflow `R` package: `tf_function()`)

```{r}
graph_fn <- tf_function(square_if_positive_verbose)

graph_fn(x)
graph_fn(x)
graph_fn(x)

graph_fn(y)
graph_fn(y)
graph_fn(y)
```

In graph mode, both branches of the `if` expression are traced into a tensorflow graph the first time the function is called and the resultant graph is cached by `tf.function()`.  Then on subsequent calls only the cached graph is evaluated.

The key takeaways are that `autograph()` helps you write natural R code and use tensors in expressions where R wouldn't otherwise accept them. And that `autograph` is smart enough to do the right thing in both eager mode and graph mode. 


### Control Flow
The most prominent and used feature of tfautograph is the fact that it will translate control flow statements. This includes `if`, `while`, `for`, `break`, `next`, and `switch`. Here is handy summary table of the translation endpoints.

```{r, echo = FALSE, results = 'asis'}
knitr::kable(matrix(
  ncol = 3, byrow = TRUE,
  dimnames =
    list(c(),  
   c("R expression", "Graph Mode Translation", "Eager Mode Translation")),
  c("`if(x)`", "`tf$cond(x, ...)`", "```if(x$`__bool__`())```",
    "`while(x)`",  "`tf$while_loop(...)`", "`while(as.logical(x))`",
    "`for(x in tensor)`", "`tf$while_loop(...)`", "`while(!is.null(x <- iter_next(tensor))`",
    "`for(x in tfdataset)`", "`Dataset$reduce()`", "`while(!is.null(x <- iter_next(dataset))`"
  ),
))
```

Lets go through them one at a time. 


#### `if`

`if` statements written in R automatically get translated to a `tf.cond()` call in graph mode. Both locally modified and/or created variables, as well as the return value of the overall expression are captured as part of the constructed `tf.cond()`. Unbalanced branches are automatically balanced to satisfy the requirements of `tf.cond()`. 

```{r}
tf_sign <- tf_function(autograph(function(x) {
  numeric_sign <- 0
  string_sign <- if (x > 0) {
    message("Tracing positive branch")
    numeric_sign <- 1
    "positive"
  } else if (x < 0) {
    message("Tracing negative branch")
    numeric_sign <- -1
    "negative"
  } else {
    message("Tracing zero branch")
    "zero"
  }
  list(numeric_sign, string_sign)
}))

tf_sign(x)
tf_sign(y)
tf_sign(tf$zeros(list()))
```

```{r}
tf_sign <- tf_function(autograph(function(x) {
  if (x > 0)
    1
  else if (x < 0)
    -1
  else
    0
}))
```

In eager mode, `if(eager_tensor)` is translated to ```if(eager_tensor$`__bool__`())```. (Essentially, a slightly more robust way to call `eager_tensor$numpy()`) 

#### `while`
In graph mode, `while` expressions are translated to a `tf$while_loop()` call.


```{r}
naive_factorial <- tf_function(autograph(function(x) {
  total <- 1
  while (x != 0) {
    message("Evaluating while body R expression")
    total %<>% multiply_by(x)
    x %<>% subtract(tf_sign(x))
  }
  total
}))

naive_factorial(x)
naive_factorial(y)
```

In eager mode, `while(eagor_tensor)` is translated to `while(as.logical(eager_tensor))`. (The `tensorflow` R package provides tensor methods for many S3 generics, including `as.logical()`).

Here is an example of an autographed `while` expression being evaluated eagerly. Remember, `autograph()` is not just for functions!

```{r}
total <- 1
x
autograph({
  while (x != 0) {
    message("Evaluating while body R expression")
    total %<>% multiply_by(x)
    x %<>% subtract(tf_sign(x))
  }
})
x
total
```

[`tf.while_loop()`](https://www.tensorflow.org/api_docs/python/tf/while_loop) has many options. In order to pass those through to the call, precede the `while` expression with `ag_while_opts()`



```{r}
naive_factorial_serial <- tf_function(autograph(function(x) {
  total <- 1
  
  ag_while_opts(parallel_iterations = 1,
                back_prop = FALSE)
  while (x != 0) {
    message("Evaluating while body R expression")
    total %<>% multiply_by(x)
    x %<>% subtract(tf_sign(x))
  }
  total
  }))
naive_factorial(y)
```


#### `for`
Autographed `for` loops build on top of while loops. Presently, `autograph` adds support for new types of values passed to `for`: tfdatasets, tensors, and (in eager mode only) python iterators 

In eager mode, both datasets and tensors are coerced to iterators (via ```iterable$`__iter__`()```, through the ergonomic wrapper `reticulate::as_iterator()`). The arguments are then iterated over until the iterable is finished. Essentially, a call like

```{r, eval = FALSE}
for(elem in iterable) {...}
```

gets translated to

```{r, eval = FALSE}
iterator <- as_iterator(iterable)
while(!is.null(elem <- iter_next(iterator))) {...}
```

Note, that tensors are iterated over their first dimension.
```{r}
m <- tf$convert_to_tensor(matrix(1:12, nrow = 3, byrow = TRUE))
m
autograph({
  for (elem in m)
    print(elem)
})
```


In graph mode, tensors are iterated over within a `tf$while_loop()`. As such,
you can pass additional options via `ag_while_opts()` just as you would to an
autographed `while()` expression.


In graph mode, datasets passed to `for` are translated to a `dataset$reduce()`
call.

```{r}
niave_reduce_sum <- tf_function(autograph(function(ds) {
  sum <- tf$zeros(ds$element_spec$shape, dtype = ds$element_spec$dtype)
  for (elem in ds) {
    message("Tracing for loop body")
    sum %<>% add(elem)
  }
  sum
}))

ds <- tensor_slices_dataset(m)
niave_reduce_sum(ds) # loop body is only traced just once on first call
niave_reduce_sum(ds)
```

Be careful to not pass a dataset that repeats forever! One way to guard against
iterating forever is to use `take()`.
 
 
```{r}
safe_niave_reduce_sum <- tf_function(autograph(function(ds, max_iter = 2) {
  sum <- tf$zeros(ds$element_spec$shape, dtype = ds$element_spec$dtype)
  for (elem in ds$take(as.integer(max_iter))) {
    message("Tracing for loop body")
    sum %<>% add(elem)
  }
  sum
}))

ds %>% 
  dataset_repeat() %>% # repeats infinitly!
  safe_niave_reduce_sum(max_iter = 6)
```

Lets tie some concepts together to write fizzbuzz! Before we do that, we'll
write a helper `knitr_log`, that will help us capture the output from `tf.print`
in this Rmarkdown vignette. (If we don't redirect output to a file, then it
would show up in the rendering console and not in the vignette)

```{r}
temp_knitr_logfile <- local({
  LOGFILE <- NULL
  function(what = c("get", "reset")) {
    if (is.null(LOGFILE))
      LOGFILE <<- sprintf("file://%s", tempfile())
    
    what <- match.arg(what)
    if (what == "reset")
      on.exit({
        unlink(LOGFILE)
        LOGFILE <<- NULL
      })
    
    LOGFILE
  }
})

print_temp_knitr_logfile <- function(reset = TRUE) {
  filepath <- temp_knitr_logfile(if(reset) "reset" else "get")
  output <- readLines(filepath, warn = FALSE)
  writeLines(strwrap(output))
}

```


```{r}
fizzbuzz <- autograph(function(n) {
  for (i in range_dataset(from = 1L, to = n)) {
    if (i %% 15L == 0L)
      tf$print("FizzBuzz", end = "", output_stream = temp_knitr_logfile())
    else if (i %% 3L == 0L)
      tf$print("Fizz", end = "", output_stream = temp_knitr_logfile())
    else if (i %% 5L == 0L)
      tf$print("Buzz", end = "", output_stream = temp_knitr_logfile())
    else
      tf$print(i, end = "", output_stream = temp_knitr_logfile())
    
    if(i < tf$cast(n-1L, i$dtype))
      tf$print(", ", end = "", output_stream = temp_knitr_logfile())
  }
})
```

First, lets run it in eager mode.
```{r}
fizzbuzz(tf$constant(25L))
print_temp_knitr_logfile()
```
And now in graph mode.
```{r}
fizzbuzz <- tf_function(fizzbuzz)
fizzbuzz(tf$constant(25L))
print_temp_knitr_logfile()
```

#### `break` / `next`

`while` and `for` both can accept `break` and `next` statements, both in eager
mode and in graph mode.


#### Passing additional options


#### Providing hints


#### Control Dependencies


#### View graph


#### Growing Objects / TensorArrays



#### How it works





# Demo -- Full MNIST Training Loop

Here is a full MNIST training loop implemented in R using tfautograph. 
(adapted from [here](https://www.tensorflow.org/beta/guide/autograph))



```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(magrittr)
library(purrr, warn.conflicts = FALSE)

library(tensorflow)
library(tfdatasets)
library(keras)

library(tfautograph)

rm(list = ls()) # clear the global env

# reticulate::use_virtualenv("tf2-nightly")

# All of tfautograph works in tf 1.14 also, but this readme expects 2.0.
tf$version$VERSION
stopifnot(tf_version() >= "2")
```



### Download data


```{r}
prepare_mnist_features_and_labels <- function(x, y) {
  x = tf$cast(x, tf$float32) / 255
  y = tf$cast(y, tf$int64)
  list(x, y)
}

mnist_dataset <- function() {
  c(c(x, y), .) %<-% tf$keras$datasets$mnist$load_data()
  tensor_slices_dataset(list(x, y)) %>%
    dataset_map(prepare_mnist_features_and_labels) %>%
    dataset_take(20000) %>%
    dataset_shuffle(20000) %>%
    dataset_batch(100)
}

train_dataset <- mnist_dataset()
```


### Define the model
```{r}
new_model_and_optimizer <- function() {
  model <- keras_model_sequential() %>%
    layer_reshape(target_shape = c(28 * 28),
                  input_shape = shape(28, 28)) %>%
    layer_dense(100, activation = 'relu') %>%
    layer_dense(100, activation = 'relu') %>%
    layer_dense(10)
  model$build()
  optimizer <- tf$keras$optimizers$Adam()
  list(model, optimizer)
}
c(model, optimizer) %<-% new_model_and_optimizer()
```


### Define the training loop
```{r, message=TRUE, warning=TRUE}
compute_loss <- tf$keras$losses$SparseCategoricalCrossentropy(from_logits = TRUE)
compute_accuracy <- tf$keras$metrics$SparseCategoricalAccuracy()

train_one_step <- function(model, optimizer, x, y) {
  with(tf$GradientTape() %as% tape, {
    logits <- model(x)
    loss <- compute_loss(y, logits)
  })

  grads <- tape$gradient(loss, model$trainable_variables)
  optimizer$apply_gradients(
    transpose(list(grads, model$trainable_variables)))

  compute_accuracy(y, logits)
  loss
}


train <- autograph(function(model, optimizer) {
  step <- 0L
  loss <- 0
  for (batch in train_dataset) {
    c(x, y) %<-% batch
    step %<>% add(1L)
    loss <- train_one_step(model, optimizer, x, y)
    if (compute_accuracy$result() > 0.8) {
      tf$print( 'Step', step, ': loss', loss, '; accuracy', compute_accuracy$result(),
        output_stream = log_file)
      # We direct all tf$print() outputs to a log file only so that Rmarkdown can
      # show output that would otherwise just go to stdout
      tf$print("Breaking early", output_stream = log_file)
      break
    } else if (step %% 10L == 0L)
      tf$print('Step', step, ': loss', loss,
               '; accuracy', compute_accuracy$result(), 
               output_stream = log_file)
  }
  list(step, loss)
})
```

### train in graph mode
```{r, message=TRUE, warning=TRUE}

log_file <- sprintf("file://%s", tempfile("TF-print-log", fileext = ".out"))

train_graph <- tf_function(train)
c(step, loss) %<-% train_graph(model, optimizer)

cat(readLines(log_file), sep = "\n")
cat(sprintf(
  'Final step %i: loss %.6f; accuracy %.6f',
  as.array(step), as.array(loss), as.array(compute_accuracy$result())))
```


### train in eager mode
```{r, message=TRUE, warning=TRUE}
# autograph also works in eager mode

log_file <- sprintf("file://%s", tempfile("TF-print-log", fileext = ".out"))

c(model, optimizer) %<-% new_model_and_optimizer()
c(step, loss) %<-% train(model, optimizer)

cat(readLines(log_file), sep = "\n")
cat(sprintf(
  'Final step %i: loss %.6f; accuracy %.6f',
  as.array(step), as.array(loss), as.array(compute_accuracy$result())))
```

